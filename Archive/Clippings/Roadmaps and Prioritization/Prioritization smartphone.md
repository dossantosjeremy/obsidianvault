

*The RICE Method ranks items by multiplying* Reach *(the number of users the item affects) by* Impact *(the result the item has on users) and* Confidence *(how much validation you have for your estimates). This resulting number is divided by* Effort *(the amount of work it will take to implement the item) to obtain an item’s final score.* 


### 3.B. Criteria 

This RICE method is based on scoring each item on 4 different dimensions:

-   **Reach**: the number of users the item affects within a given time period 
-   **Impact**: the value added to users 
-   **Confidence**: how confident you are in your estimates of the other criteria (for example, highly confident if multiple data sources support your evaluation) 
-   **Effort**: the amount of work necessary to implement the item 

### 3.C. Process

Using the RICE method is straightforward. Separate scores are assigned for each criterion, then an overall score is calculated. 

-   A reach score is often estimated by looking at the number of users per time period (e.g., week, year);  ideally, this number is pulled from [digital analytics](https://www.nngroup.com/articles/analytics-user-experience/) or [frequency metrics](https://www.nngroup.com/articles/frequency-recency/). 
-   The impact score should reflect how much the item will increase delight or alleviate friction; it is hard to precisely calculate, and, thus, it’s usually assigned a score (for example, through voting, like in the previous methods) often on a scale from .25 (low) to 3 (high).  
-   The confidence score is a percentage that represents how much you and your team trust the reach and impact scores.  100% represents high confidence, while 25% represents wild guesses. 
-   The effort score is calculated as “person-months” — the amount of time it will take all team members to complete the item. For example, an item is 6 person-months if it would require 3 months of work from a designer and 1 month from 3 separate developers.  






## 4\. MoSCoW Analysis

### 4.A. Overview

MoSCoW analysis is a method for clustering items into four primary groups: *Must* *Have*, *Should* *Have*, *Could* *Have*, and *Will* *Not* *Have*. It was created by Dai Clegg and is used in many Agile frameworks. 

![MoSCoW uses 4 categories (Must Have, Should Have, Could Have, and Will Not Have) to group and prioritize items.](https://media.nngroup.com/media/editor/2021/11/16/moscow-nng.png)

*MoSCoW analysis groups items into four groups:* Must have *(items that are essential to the project),* Should have *(items that are very important, but not essential),* Could have *(items that are nice to have), and* Will not have *(items that aren’t needed).* 

### 4.B. Criteria

This prioritization approach groups items into four buckets: 

-   **Must have**: items that are vital to the product or project. Think of these as required for anything else to happen. If these items aren’t delivered, there is no point in delivering the solution at all. Without them the product won’t work, a law will be broken, or the project becomes useless. 
-   **Should have:** items that are important to the project or context, but not absolutely mandatory. These items support core functionality (that will be painful to leave out), but the project or product will still work without them. 
-   **Could haves**: items that are not essential, but wanted and nice to have. They have a small impact if left out. 
-   **Will not have:** items that are not needed. They don’t present enough value and can be deprioritized or dropped. 

### 4.C. Process

MoSCoW analysis can be applied to an entire project (start to finish) or to a project increment (a sprint or specific time horizon). 

Begin by identifying the scope you are prioritizing items for. If your goal is to create a UX roadmap, you’ll usually have to prioritize for the first three time horizons: now (work occurring in the next 2 months), next (work occurring in the next 6 months), and future (work occurring in the next year). 

Compile the items being prioritized and give each team member 3 weighted voting dots, (one dot with a 1 on it, the next with a 2 on it, and so forth). Ask team members to assign their dots to the items they believe most important, with 3 being weighed most heavily.

![Each team member places weighted votes, resulting in scores for each item.](https://media.nngroup.com/media/editor/2021/10/29/moscow-voting.png)

*Team members vote on the items that they believe are* Must Have *for the roadmap time horizon. In the example above, each team member is given three voting dots, one with a 1, one with a 2, and one with a 3. They place their dots on the three items they believe should have the highest priority (with 3 being the item of highest priority among the 3).* 

Add up each item’s score based on the ranked votes (3 = 3 points and so forth). Identify the items with the highest scores and make sure that everybody in the group agrees on their importance. 

As each item is discussed and agreed upon as a *Must* *Have*, move it to a new dedicated space. Repeat this process for lower-priority items and assign them to the *Should* *Have,* Could *Have*, and *Will* *Not* *Have groups* based on their scores.

Once you have assigned each item to one of the four groups, establish the resources and bandwidth required for each group, starting with the *Must* *Haves*. Keep track of the total bandwidth and resources at your disposal, distributing and allocating your total amount across *Must* *Haves* (which should get the most resources), *Should* *Haves* (with the second most resources), and finally *Could* *Haves* (with few resources)

4.D. Best for Teams with Clear Time Boxes

MoSCoW is a good prioritization method for teams looking for a simplified approach (given the relatively vague prioritization criteria set) and with a clear time box identified for the work.

The Kano model scores items based on satisfaction and functionality. Using these scores, items can be clustered into 4 groups:* Attractive*,* Performance*,* Indifferent*, and* Must be*.* 

### 5.B. Criteria 

This prioritization method uses two primary criterions to rank items: functionality and satisfaction. 

-   **Functionality** represents the degree to which the item can be implemented by the company. It can have 5 possible values ranging from -2 to 2:
    -   **None (-2)**: the solution cannot be implemented
    -   **Some (-1)**: the solution can be partly implemented
    -   **Basic (0)**: the solution’s primary functions can be implemented, but nothing more 
    -   **Good (1)**: the solution can be implemented to an acceptable degree
    -   **Best (2)**: the solution can be implemented to its full potential 
-   **Customer satisfaction** for each item is also assessed on a spectrum from -2 to 2:
    -   **Frustrated (-2)**: the solution causes additional hardship for the user
    -   **Dissatisfied (-1)**: the solution does not meet users’ expectations
    -   **Neutral (0)** 
    -   **Satisfied (1)**: the solution meets users’ expectations
    -   **Delighted (2)**: the solution exceeds users’ expectations

### 5.C. Process

Each item is first assigned a satisfaction score and a functionality score. The satisfaction score should be based on user data — for example, on existing user research or on a top-task user survey asking users to rate the importance of each feature; the functionality score can be rooted in the collective expertise of the team.  

These scores are then used to plot items onto a 2D-graph, with the x-axis corresponding to functionality and the y-axis to satisfaction. Each axis goes from -2 to 2. 

Based on their placement on their scores, items fall into one of four categories: 

1.  The **Attractive** category (often called *Excitement*) are items that are likely to bring a considerable increase in user delight. A characteristic of this category is the disproportionate increase in satisfaction to functionality. Your users may not even notice their absence (because they weren’t expectations in the first place), but with good-enough implementation, user excitement can grow exponentially. The items in the *Attractive* are those with a satisfaction score of 0 or better. These items appear above the blue *Attractive* line in the Kano illustration above.
2.  The **Performance** category contains items that are utilitarian. Unlike other categories, this group grows proportionately. The more you invest in items within this category, the more customer satisfaction they are likely to prompt. The items in the *Performance* category have equal satisfaction and performance scores and fall on the green line in the Kano illustration above.  
3.  The **Indifferent** category contains items that users feel neutral towards — satisfaction does not significantly increase or decrease with their functionality and is always 0. Regardless of the amount of investment put into these items, users won’t care. These items are all placed on the dark blue *Indifference* line (which overlaps with the x-axis). 
4.  The **Must-be** category are basic items that are expected by users. Users assume these capabilities exist. They are unlikely to make customers *more* satisfied, but without them, customers will be disproportionately dissatisfied. Items fall into the Must-be category when their satisfaction score is 0 or worse. These are the items in the purple area of the Kano diagram, below the purple *Must Be* line.

Once items are assigned to groups, make sure that everybody in the team agrees with the assignment. Items with scores of (0,0), (-2,0) and (+2,0) may initially belong to two groups. In these cases, discuss the item and ask yourself if user value will grow proportionately with your team’s investment. If the answer is yes, group the item with *Performance*. In cases this is false, group the item with *Indifferent*. 

Move items as needed, then prioritize items into your roadmap. Items in the *Performance* category should have the highest priority, followed by *Must* *be*, *Attractive*, then *Indifferent*. 

Feasibility, Desirability, and Viability Scorecard 

### 2.A. Overview

This method was developed by IDEO in the early 2000s. It ranks items based on a sum of individual scores across three criteria: feasibility, desirability, and viability. 

![A table with items in each row and the criteria in each column. Totals are calculated for each item.](https://media.nngroup.com/media/editor/2021/10/29/feasibility-desirability-viability-scorecard.png)

*Create a table where items’ individual scores can be documented and added for a total score. Total scores are then compared, discussed, and reorganized to determine the final prioritization. The items with the highest overall scores best satisfy the prioritization criteria (in this case, desirability, feasibility, and viability).* 


### 2.B. Criteria 

This prioritization method uses three criteria to rank items (i.e., features to be implemented):

-   **Feasibility**: the degree to which the item can be technically built. Does the skillset and expertise exist to create this solution?
-   **Desirability**: how much users want the item. What unique value proposition does it provide? Is the solution fundamentally needed, or are users otherwise able to accomplish their goals? 
-   **Viability**: if the item is functionally attainable for the business.  Does pursuing the item benefit the business? What are the costs to the business and is the solution sustainable over time? 

### 2.C. Process

Create a table, with one row for each possible item, and columns for the 3 criteria — feasibility, desirability, and viability. Then, determine a numeric scoring scale for each criterion. In the example above, we used a numeric scale from 1 to 10, with 1 being a low score. 

Next, give each item a score across each criterion. Scoring should be as informed as possible — aim to include team members who have complementary expertise. Once each item is scored across each criterion, calculate its total score and force a rank. Sort the table from highest to lowest total score, then discuss the results with your team. 


D. Best for Customized Criteria 

This scorecard format is highly customizable. You can add columns to reflect criteria specific to your organization’s context and goals. You can also replace the criteria with others relevant to you. For example, the [NUF Test](https://gamestorming.com/nuf-test/), created by Dave Gray, uses the same scorecard format, but with *New*, *Useful*, *Feasible* as the criteria set. 

# roadmap

A [UX roadmap](https://www.nngroup.com/articles/ux-roadmaps/) is a strategic, living artifact that aligns, prioritizes, and communicates a UX team’s future work and the problems it needs to solve. Roadmaps are successful when they make realistic promises, value functionality over pretty visuals, or are strategic documents instead of feature-specific release plans. 

## The 6 Steps of Roadmapping

Successful roadmapping include 6 key high-level steps:

1.  **Establish goals:** Determine the purpose of your roadmap and who should be involved.
2.  **Gather inputs:** Collect problems that need to be solved from stakeholders and existing research.
3.  **Create themes:**  Cluster problems into into themes. 
4.  **Prioritize themes:** Establish criteria and create a scoring framework to rank themes.
5.  **Visualize and share:** Plot themes into a timeline and distribute the resulting visualization (your roadmap).
6.  **Revisit and update:** Routinely return to your roadmap and make adjustments as necessary. 

### **1A. Why: Determine Your Objective** 

The first question to ask is:  What is the primary purpose of this roadmap? A roadmap can have different goals:  

-   Increase team awareness
-   Enable crossdisciplinary visibility and alignment 
-   Educate others on the UX process 
-   Manage bandwidth and resource allocation  
-   Show to stakeholders what problems you aim to solve
-   Prioritize future work 
-   Define a UX vision 
-   Identify a [minimum viable product (MVP)](https://en.wikipedia.org/wiki/Minimum_viable_product) 

Roadmaps can become political, complex artifacts. A single, primary purpose for your roadmapping initiative will keep it focused and will establish clear priorities (for example, what information to communicate, who is involved in the process, or the tool the roadmap lives in). 

### **1B. What: Identify a Scope**

Determine the [scope of your roadmap](https://www.nngroup.com/articles/roadmap-types/). Roadmaps that include UX work can have 3 scopes: product, discipline, and specialty. Identify which scope is best for your objective. If you will create a discipline or specialty roadmap, clearly establish what product(s) the roadmap will serve. Both these types of roadmaps roadmaps can visualize work across several products within a portfolio (the broadest approach) or across just one specific product-area within a single product (the lowest-granularity approach). 


1C. Who: Establish a Core Team**

With your primary goal and determined scope in mind, find support. Pull together a crossdisciplinary team that has insight or responsibility over the future work’s completion. Establish stakeholder support for the roadmapping initiative by educating managers or executives on the benefits of the process. Depending on the maturity of your organization (or lack thereof), you may need to explain the value of a roadmap, what you hope to accomplish, and ask these stakeholders to support the initiative among their respective groups.















