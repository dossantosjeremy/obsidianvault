# Resources

[[Obsidian/01 üíº Projects/AI For Designers/Resources#^368c75|Resources - Tools for Ethical AI]]

# Introduction 
* Human bias: Bias that is rooted in beliefs, attitudes and prejudices of individuals. 
* Occurs when people have incomplete information. 
* Anchoring Bias: People looking at one piece of information only instead of looking at all types of information objectively. 
* Confirmation bias: Tendency to look for or selectivelt interpret information that confirms ndividual beliefs. 

# Video Notes

* Levels of bias:
	* Systemic Bias
	* Statistical and computational 
	* Human bias
* Biased AI can lead to unfair or discriminatory outcomes. 
* Discriminatory outcomes
	* From minor annoyances like voice recognition that struggles with accents
	* To unfair ones: Unfair hiring processes
* We can address it by focusing on values. We need to align on the values we want to reflect. 
* We can address computational bias through clean data
* Make sure it does not perpetuate bias. 
* Addressing human bias 
	* Through transparency
	* Transparency: related to explainable AI
	* The capacity to express why a particular AI system reached a particular decision, recommendation or prediction
	* We can advocate and evangelize them. 
	* Design conversations matter. 
* Fighting bias
	* Conducting diverse and inclusive user research
	* Understand the needs, context and backgrounds of various users to ensure the design meets needs and does not favor a group over another. 
	* Favor a collaborative design by including users especially from the under represented group
	* It will promote a more equitable design decision
	* Establish ethical guidelines for AI within the design inside the organization. 
	* Creating a more inclusive, fair and more equitable digital world. 
	* It is about setting standards for the use of AI
	* We can leverage anti-bias strategies in our processesby leveraging AI in our processes. 

**The types of bias**
![A Figure that represents categories of AI bias. From the article "Towards a Standard for Identifying and Managing Bias in Artificial Intelligence" by Reva Schwartz, Apostol Vassilev, Kristen K. Greene, Lori Perine, Andrew Burt, Patrick Hall](https://public-images.interaction-design.org/courses/lesson-materials/04-02-02-nist.jpg)


## Types of bias

### Institutional bias
* Also called historical bias*
* Embedded within systems, institutions and organizations. 

## Statistical and computational bias
* Computational bias often reveals a systemic bias
* When the sampled data does not represent the population
* Artificial intelligence produce biased results due to 
	* Biased data
	* Biased training
	* Algorithmic design flaws


## How can designers deal with bias

### Working with bias
* Machine learning models are inherently biased as they are fed with data that is biased in the society
* The data that has been fed to the machine learning model should be representative and inclusive*

### Safety and security
- We have to be aware of the safety issues when using AI
	- Example: Microsoft Bing being offensive 
- Data privacy: Data being dimantled, exposed or stolen. 
- Prompt injection: Following instructions provided by a malicious user. 
- Weaponized tools: AI can produce tools that can be used by authoritarian governments 
	- Allowing widespread surveillance
	- Deepfakes spreading information 
- Dependance on AI systems
	- Safety risks if it fails or makes mistakes
	- True in healthcare or transportation where errors can have consequences. 
	- Adding safety scafolding to the way the interaction with AI systems is designed. 
## Ways in which we can protect the user

### Steps that can be taken
1. Recognizing bias
2. Incorporate safety features in the interface
	1. Users should report errors, biases and abuse within the design 
	2. If the result is purely AI-generated: Allow users to configure different parameters and to regenerate results
3. Involve the community
	1. Having a team to review and moderate content. 
	2. Incentivize users to be in the first line of defense
4. Use AI to counter AI
	1. Companies that developed algorithms to counter bias in AI

[[Obsidian/01 üíº Projects/AI For Designers/Resources|Resources]]


1.  **[Fairlearn](https://fairlearn.org/)**: An open source toolkit from Microsoft for data scientists and developers to **assess** and improve the fairness of their AI systems.
        
    2.  **[Accenture‚Äôs AI testing services](https://newsroom.accenture.com/news/accenture-launches-new-artificial-intelligence-testing-services.htm)**: Rely on a ‚ÄúTeach and Test‚Äù methodology to **train** AI systems to avoid biases.
        
    3.  **[Bias Analyzer](https://www.pwc.com/us/en/tech-effect/ai-analytics/artificial-intelligence-bias.html)**: A cloud-based application by PwC that flags potential biases in AI **outputs**.
        
    4.  **[FairML](https://dspace.mit.edu/handle/1721.1/108212)**: A toolbox developed by MIT student Julius Adebayo for auditing predictive models by analyzing the model's **inputs**.
        
    5.  **[Google's What-If tool](https://pair-code.github.io/what-if-tool/ai-fairness.html)**: This tool questions what fairness means and allows product developers to sort the data according to a different type of fairness, allowing humans to see the trade-offs in different ways to measure fairness and make decisions accordingly.¬†

### Manage data to prevent computational bias  
1. Start with clean data
2. Understand why you need the data and how will the AI system help the user
3. Identify what type of user data is relevant for the AI system
4. Examine the dataset for potential biases using statistical tools and models
5. Identify gaps in the data and how we can source the information
6. Explain the users why we collect the data and allow them to opt out of sharing the information
7. Store the data securely and communicate the security policies and measures with the user. 
8. Format the data consistently
9. Continuously update the dataset with live data
10. Keep testing the AI system

