---
category: "[[Clippings]]"
author: "[[The Interaction Design Foundation]]"
title: "Lesson 4.7 - AI for Designers"
source: https://www.interaction-design.org/courses/ai-for-designers/lessons/4.7
clipped: 2024-05-10
published: 
topics: 
tags: [clippings]
---

Estimated time to complete: 21 mins

![An illustration that represents humanity-centered design. There are three circles inside of one another. The smallest represents people-centered design, the next human-centered design and the third, largest represents humanity centered design.](https://public-images.interaction-design.org/courses/lesson-materials/04-07-01-human-centered-design.jpeg)

*There’s a mix between excitement and skepticism around any new ground-breaking technology. AI is no different. It’s easy to get carried away and build AI-applications just because we can. But, as AI Product Designer Ioana Teleanu explains in this video, we must consider the larger picture and the impact of our work not just on our immediate audience, but humanity in general.* 

**

Show Hide video transcript

1.  00:00:00 --> 00:00:32
    
    Now more than ever, it's essential to keep  people at the center of the design process. With the extreme pace at which technology is evolving, it's easy to get swept away into this mindset of   technology enthusiasm, where we're all building solutions just because they're possible. We should stop and ask, "Should we build this? Does anyone need this?"
    
2.  00:00:32 --> 00:01:00
    
    And if the answer is 'yes', what are the ethical implications of the design decisions we're  making? How do we make sure that our solution is inclusive, safe, sustainable? The motivations behind technological progress and economic competition are far more powerful than the few important voices drawing attention and sounding alarms around the ethics – or lack of them – in the AI space.
    
3.  00:01:00 --> 00:01:31
    
    It is our role as designers – and I want to empower each and every one – to be the person in the room that questions the excitement around a particular AI possibility and interjects the conversation with the question: "Is this useful? Do people need it? How do we prevent it from causing harm?" – and so on. And here's another interesting take: maybe it's high time we shift from human-centered design
    
4.  00:01:31 --> 00:02:03
    
    to humanity-centered design or life-centered design. What this means is that we should expand our thinking beyond considering how our design decisions impact people, further – onto how they're impacting the \*planet\*. We should focus on the entire ecosystem of people, all living things, and the physical environment and solve the root issues, not just the problem as presented, which is often a symptom, not the cause. We should all take a \*long-term systems point of view\*.
    
5.  00:02:03 --> 00:02:18
    
    We must realize that the impact of  our actions on society and the ecosystem can take years to appear, or manifest even decades later. Design \*with\* the community, not for them.
    

**

One of the biggest examples of the consequences of design decisions, and particularly with AI, is social media. We live in an extremely polarized society, with individuals living in a narrow echo chamber, unable to tolerate even slightly differing viewpoints. 

In addition, people live under constant pressure to lead “perfect lives”. Several studies have concluded that Instagram alone has adversely affected mental health.

Designers, engineers, and now, data scientists wield a mighty powerful weapon, and we must work together to build a happy world worth living in.

## Human-Centered AI: A Framework for Designer-Data Scientist Collaboration

Josh Lovejoy, a UX designer, and Jess Holbrook, a UX manager and researcher at Google’s People+AI Research (PAIR) team, co-wrote a step-by-step framework for designing with machine learning. Here’s a slightly edited and adapted version of their framework. Notice that it has parallels with traditional human-centered design.

1.  **Don’t expect AI to figure out what problems to solve.** No machine can replace humans. Our capacity to empathize with people is our superpower. AI can help you with research; leverage its scalability and processing power to augment your research effort and identify the root cause(s) of problems. 
    

**

Get your free template for “The 5 Whys Method”

![The 5 Whys Method](https://public-images.interaction-design.org/templates/thumbnails/template_caab700a-74fa-4a8b-bc24-2fcdb41ddc39.png) ![The 5 Whys Method](https://public-images.interaction-design.org/templates/thumbnails/template_caab700a-74fa-4a8b-bc24-2fcdb41ddc39.png)

Get your free template for “The 5 Whys Method”





**

2.  **Ask yourself if AI will address the problem in a unique way.** The first rule during ideation is to come up with solutions for problems. We start with the user and their problems. Technology is only an enabler, not the goal. Consider the holistic solution and then identify if and how AI can support it.
    
3.  **Fake it with wizards and personal examples.** Prototyping and testing with AI can be tricky, especially if your solution will rely on personalization. Two methods that come in handy are Wizard of Oz prototyping and testing with personal examples.
    
    For testing prototypes where the algorithm is not ready, You can fake the AI with a real human acting in place of the AI. This can help you observe how users interact with a system that they believe is artificial.
    
    When you do have your first versions of AI models ready, ask your research participants to bring in their personal data (non-sensitive, and with consent!) to understand how your system behaves with realistic data. 
    
4.  **Weigh the costs of false positives and false negatives.** No system is perfect. What matters most are the consequences of errors. In the case of AI predictions, there can be 4 possible scenarios, as represented by this 2x2 matrix:
    
    -   True positive: AI correctly predicts a positive outcome.
        
    -   False negative: AI wrongly predicts a negative outcome.
        
    -   False positive: AI wrongly predicts a positive outcome.
        
    -   True negative: AI correctly predicts a negative outcome.
        
    
    An animated gif that shows AI predictions, both positive and negative.
    
    © Interaction Design Foundation, CC BY-SA 4.0
    
    AI predictions may predict positive or negative outcomes. As long as these predictions are correct (true positives and true negatives), there isn’t anything to worry about. But how costly would it be for the user for these predictions to be false (false positives and false negatives)? The confusion matrix plots all these outcomes in a 2x2 matrix to make it easier to analyze the performance of algorithms.
    
    To a machine, all errors are equal. People will perceive different types of errors differently. For example, if a smartphone calls the wrong phone number, the user might suffer mild embarrassment. But if a self-driving car takes a wrong turn, it may even physically harm the user.
    
5.  **Plan for co-learning and adaptation.** AI (and humans) continuously evolve. Hence, we must factor these into the models we design and build. However, users may form mental models for how the AI might behave and have very different expectations. Hence, whenever we make changes to the model, we must communicate those to the user as well. As Microsoft’s HAX Toolkit guidelines state:
    
    1.  Learn from user behavior.
        
    2.  Update and adapt cautiously.
        
    3.  Encourage granular feedback.
        
    4.  Notify users about changes.
        
6.  **Teach your algorithm using the right labels.** To an algorithm, a picture, video, audio or word is a bundle of data. Data labeling for AI refers to the process of annotating or tagging data samples to create labeled datasets. These labeled datasets are used to train machine learning models, especially in supervised learning tasks. In supervised learning, an AI model learns to make predictions or classifications based on labeled examples provided during the training phase. Data labeling—typically done by human annotators—is a crucial step in the development of AI systems, as the quality and richness of the labeled data directly influence the AI model's ability to generalize and perform well on unseen data.
    

![Four versions of a photo of Abraham Lincoln. It represents a matrix of pixel values. The photo on the right is clear as the original while each one after becomes more pixelated.](https://public-images.interaction-design.org/courses/lesson-materials/04-07-02-pixel-matrix.jpg)

Machines don’t see the world the same way as humans do. So, ensure your training data has the right labels.

© Interaction Design Foundation, CC BY-SA 4.0

7.  **Extend your design family; AI is a creative process.** When designing for AI, work alongside data scientists, instead of instructing them.
    

## The Take Away

In the context of AI, human-centered design is not that different from traditional design; it means to craft technologies with people at its core. You should understand your users deeply, consider their values, and, most importantly, acknowledge the ethical impact of AI decisions. As designers, your responsibility is to ensure that AI benefits society positively. Empathy becomes a guiding principle to help you design AI systems that not only function well technically, but also respect human perspectives, are inclusive and avoid bias. 

In essence, human-centered design in the age of AI strives to make technology work for people, enhance human experiences and ensure ethical interactions with artificial intelligence.

## References and Where to Learn More

Learn about humanity-centered design and how to design *with* the community, not for them, in the course, **[Design for a Better World with Don Norman](https://www.interaction-design.org/courses/design-for-a-better-world-with-don-norman-course)**.

Read Jess Holbrook’s and Josh Lovejoy’s article, **[Human-Centered Machine Learning](https://medium.com/google-design/human-centered-machine-learning-a770d10562cd)**.

During the early days of AI, Josh Clark presented a set of guiding principles for designers to design data-driven products. The principles in this talk, **[Design in the Era of the Algorithm](https://bigmedium.com/speaking/design-in-the-era-of-the-algorithm.html)** stand true even as the technology has matured.

BBC’s report on **[Instagram being sued over harm to young people's mental health](https://www.bbc.com/news/business-67207829)**. 

See Laura Moreton and Sheila Greenfield’s research on **[University students’ views on the impact of Instagram on mental wellbeing](https://doi.org/10.1186/s40359-022-00743-6)**. 

Josh Lovejoy shares how human-centered design is still at the core of working with AI in this article, **[The UX of AI](https://design.google/library/ux-ai)**. 

**[Google’s Responsible AI Practices](https://ai.google/responsibility/responsible-ai-practices/)** offers recommendations related to Human-Centered Design, Fairness, Interpretability, Privacy, Safety and Security.

Hero image: © Interaction Design Foundation, CC BY-SA 4.0

## Answer Questions to Get Your Certificate

Why is it important to answer these questions?

-   You’ll significantly improve your **ability to remember** what you’ve just learnt
-   You get closer to **your Course Certificate**
-   Get a **distinction on your certificate** when you score 90% and higher
-   Research shows that when you answer questions, you’ll greatly improve your ability to transfer knowledge into new contexts, such as your **current or future workplace**.