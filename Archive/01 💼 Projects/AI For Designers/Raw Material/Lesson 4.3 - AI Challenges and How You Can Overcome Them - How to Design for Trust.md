---
category: "[[Clippings]]"
author: "[[The Interaction Design Foundation]]"
title: "Lesson 4.3 - AI for Designers"
source: https://www.interaction-design.org/courses/ai-for-designers/lessons/4.3
clipped: 2024-05-08
published: 
topics: 
tags: [clippings]
---

*As a designer, designing for trust in Artificial Intelligence (AI) products is paramount. AI presents unique challenges that require transparent interfaces, clear feedback, and ethical considerations to build user confidence. If you prioritize trust, you can ensure user adoption and satisfaction, which enhances the overall user experience in AI-powered products.*

As AI Product Designer, Ioana Teleanu, talks about in this next video, AI can hallucinate! How can designers ensure our AI-enabled solutions are reliable and users can trust them? Let’s find out.

Show Hide video transcript

1.  00:00:00 --> 00:00:31
    
    AI can hallucinate and its behavior cannot be predicted most of the time. We can't anticipate what the user will be presented with as we can in a traditional interaction system where we design the interactions and the messaging step by step conventional user experiences are controllable. We understand and decide what happens next.
    
2.  00:00:31 --> 00:01:00
    
    With AI, we have to accommodate surprises and equally important, communicate the risk for upfront setting the right expectations about performance, indicating whether the product learns over time, clarifying that mistakes will happen and that user input will teach the product to perform better and so on. Don't omit this transparent initial communication as AI systems operate with uncertainty, and if your users expect deterministic
    
3.  00:01:00 --> 00:01:31
    
    behavior from a probabilistic system, their experience will be degraded. Another major problem is trust and transparency. People have a hard time trusting objects that feel magic. We can't trust what we can't understand. AI systems are not transparent to us for multiple reasons. For once, most of us lack the technical knowledge to actually understand what goes on under the hood. Second, and more importantly, many times with generative AI,
    
4.  00:01:31 --> 00:02:03
    
    we have no idea where the information is coming from. Tools like GPT and Bard have the power of constructing answers that feel legitimate and sound very pertinent, but can be entirely made up and inaccurate. AI doesn't tell us how they've constructed an answer which would help. And maybe as designers, we should push for this transparency in generative AI, exposing a high level thinking process that gives us the information sources
    
5.  00:02:03 --> 00:02:31
    
    and general reasoning that's behind an answer. Another reason that adds to the mistrust is how AI is communicated in the media. A Google algorithm that classifies people of color as gorillas, a Microsoft chatbot that decides to become a white supremacist in less than a day, a Tesla car operating in autopilot mode that resulted in a fatal accident. We've seen these isolated but terrible experiences. Sometimes the AI systems are described as black boxes,
    
6.  00:02:31 --> 00:03:01
    
    so maybe the solution is opening them up. Companies such as Google, Airbnb and Twitter already released transparency reports about government requests and surveillance disclosures. A similar practice for AI Systems could help people have a better understanding of how algorithmic decisions are made. The last problem I want to mention is ownership and intellectual property. This, for me is a fascinating, a very important debate.
    
7.  00:03:01 --> 00:03:32
    
    I want to start by saying that with so much AI generated art, what we'll all witness is a significant shift in what we value as a society. And to illustrate this argument, I want to give the example of Mona Lisa. Borrowing this from Mark Rolston. If you think about it, AI could basically recreate a one on one replica of Mona Lisa without any identifiable differences. Could actually make it better, more symmetrical, more technically impressive, and so on.
    
8.  00:03:32 --> 00:04:01
    
    But it won't be Mona Lisa. If we know and in the future, AI art will probably be labeled as per regulations that are not yet in place but are needed. Adobe is already adding those labels. More will follow If we know that it has been generated by AI and we look at it, we probably won't feel too much. But if we're in front of Mona Lisa at the Louvre, we know this was made by da Vinci 500 years ago.
    
9.  00:04:01 --> 00:04:33
    
    We can see the fascination of the people around us, their excitement. We ourselves can experiment our own version of interpreting and looking at it. We're humbled by being in front of this work of art that has been worshiped by humanity for hundreds of years. It has the potential of being an almost religious, fundamentally human and very touching experience, which is very unlikely to be experienced towards AI art. We will value what is human made.
    
10.  00:04:33 --> 00:05:03
    
    We will know that something was created from a person's experience, suffering, imagination, hope, scarcity is not what will make art valuable. Its creator is. And then there's another layer to this debate. If AI creates art based on everything it knows from the work of other artists is that such a different expression of creativity from that of a person who has been in art school, studied Picasso, Matisse, Mondrian.
    
11.  00:05:03 --> 00:05:23
    
    And then their style is influenced by the art history and the works they studied. Something to think about. I'm not saying theft is acceptable, which takes us back to the need for a more transparent, cited, source exposing system for generative AI. But I want to give a different angle on how art is built.
    

> *“We all fear what we do not understand.”*
> 
> *― Dan Brown, The Lost Symbol*

The best way to build trust with our users is to be as transparent as possible (without overwhelming the user with too much technical information).

1.  Clearly communicate:
    
    1.  Where does your system get its data? Indicate sources where possible.
        
    2.  What user-generated information does the system use? For example, does the system rely on other users to provide data? 
        
    3.  How does your system learn from user data?
        
    4.  What are the chances of errors?
        
2.  If your system relies on personal data (such as location data, demographic information or web usage metrics):
    
    1.  Always collect this information with full consent.
        
    2.  Ask users to explicitly opt-in to share information instead of asking them to turn off the setting.
        
    3.  Allow the user to use your solution without providing any personal data.
        

## Characteristics of a Trustworthy AI System

The National Institute of Standards and Technology (NIST) defines seven characteristics of a trustworthy AI system:  

1.  **Valid and reliable:** Validity refers to the system’s ability to meet user needs. Reliability refers to the system’s ability to keep performing, without fail. To ensure your AI products are valid and reliable, define success criteria and metrics to measure the performance of the system. Constantly assess the system to confirm it is performing as intended.
    
2.  **Safe:** AI systems must never cause harm to their users. Rigorously test and simulate real-world usage to detect possible use cases where the system may cause harm and address them through design. Designers, data scientists and developers must work together to safeguard user safety. For example, you may prohibit a user from performing certain actions based on their age or location, or display warnings prominently. 
    
3.  **Secure and resilient:** A system is resilient if it can continue to perform under adverse or unexpected conditions and degrade safely and gracefully when this is necessary. For example, you might design a non-AI-based solution to allow the user to continue using the solution in case the AI system breaks down. 
    
4.  **Accountable and transparent:** Transparency refers to the extent to which users can get information about an AI system throughout its lifecycle. The more transparent a system is, the more likely people are to trust it. For example, the system can provide status updates on its functioning or information on its process so that people using the system can understand it better.
    
5.  **Explainable and interpretable:** An explainable system is one that reveals how it works. The system can offer descriptions tailored to users’ roles, knowledge, and skill levels. Explainable systems are easier to debug and monitor.
    
6.  **Privacy-enhanced:** Privacy refers to safeguarding users’ freedoms, identities and dignity. There is a tradeoff between enhanced privacy and bias. Allowing people to remain anonymous can limit the inclusive data needed for AI to function with minimal bias.
    
7.  **Fair with harmful bias managed:** Fairness relates to equality and eliminating discrimination. Bias isn’t always negative. Fairness is a subjective term that differs across cultures and even specific applications.
    

Use this checklist to check the reliability of your AI tool.

Get your free template for “Check Your AI Tool's Reliability”

![Check Your AI Tool's Reliability](https://public-images.interaction-design.org/templates/thumbnails/template_2e99e094-884f-4a11-90a1-6346ed54f811.png) ![Check Your AI Tool's Reliability](https://public-images.interaction-design.org/templates/thumbnails/template_2e99e094-884f-4a11-90a1-6346ed54f811.png)

Get your free template for “Check Your AI Tool's Reliability”

## Unsupervised: AI Art that Sidesteps the Copyright Debate

Generative AI can create stunning works of art. *Unsupervised,* part of artist Refik Anadol’s project, called Machine Hallucinations, is a generative artwork. The abstract images are driven by the Museum of Modern Art’s (MoMA) data, guided by machine learning and intricate algorithms, showcasing the intersection of art and cutting-edge AI research.

Anadol trained a unique AI model to capture the machine's "hallucinations" of modern art in a multi-dimensional space—data was collected from MoMA’s extensive collection and processed with machine learning models.

This project tackles the challenges of AI-generated art—it has huge potential for creative expression, but it raises concerns with transparency and ethics. Anadol's work invites a conversation about the interplay between art, AI research, and technology's far-reaching impact. 

The art copyright debate centers on attributing creative rights in AI-generated artworks. Traditionally, copyright law is based on human authorship. *Unsupervised* addresses this issue by openly acknowledging the collaborative role of its AI model, StyleGAN2 ADA, in creating the art. This approach avoids copyright complexities by recognizing both the AI and the human artist, Refik Anadol, as co-creators. In doing so, *Unsupervised* fosters a shared authorship model, providing transparency and clarity in navigating the evolving landscape of art copyright for AI-generated works. 

Show Hide video transcript

1.  00:00:00 --> 00:00:30
    
    What comes to your mind when you think  about AI? Is it the vision of a new evil tech conquering the world or a friendly robot that makes our lives convenient? What about an artist? If you've used tools like Midjourney or DALL-E, then you've already encountered one form of generative AI. Generative AI is a class of artificial intelligence algorithms and models designed to generate new content, such as images, text, music and more.
    
2.  00:00:30 --> 00:01:01
    
    Generative AI operates by learning patterns and structures from large datasets: the training data. Then, when we prompt it, it tries to deliver exactly what we ask for. The output is similar to the training data, but not exactly the same. This generative AI has raised eyebrows and stirred a controversial discussion that cuts across industries: art, technology and copyright laws. On one hand, you can make the argument that technology makes art more democratic.
    
3.  00:01:01 --> 00:01:32
    
    Using AI, anyone with a vision can create art without having the traditional skills to execute it. On the other hand, artists whose work is used to train the AI, often without their permission, can argue that it's a copyright infringement, particularly if the output of the AI system competes with the training dataset. The generative AI we're most familiar with  creates images based on what we humans ask for, using the training set that we provide.
    
4.  00:01:32 --> 00:02:00
    
    But what if we can create art that looks nothing like its training data, in other words, truly original art? Not only compositions based on human-imagined prompts. What would a machine dream about after seeing the entire public collection of the Museum of Modern Art (MoMA)? AI artist Refik Anadol explored this question. In collaboration with data scientists, he tested various AI models and their studio-trained AI
    
5.  00:02:00 --> 00:02:32
    
    to create original art. The team at Refik Anadol Studio trained the  AI using the publicly available data of the MoMA collection in its 200-year history, from Van Gogh and Pablo Picasso, to even images of Pac Man. They used 380,000 extremely high-resolution images taken from more than 180,000 art pieces. It's astonishing that it looks nothing like its inputs. If you've never heard of it, you have to check it out.
    
6.  00:02:32 --> 00:03:00
    
    It's overwhelming, mind-blowing. The result is a large 24x 24-foot or 7.5 x 7.5-meter installation, which is called 'Unsupervised'. 'Unsupervised' is a constantly changing animation of abstract visuals based on the machine's interpretation of MoMA's collection. MoMA writes that: "It reimagines the history of modern art and dreams about what might have been
    
7.  00:03:00 --> 00:03:30
    
    and what might be to come." When we experienced this piece of art, we had no doubt that it was indeed a completely new style of art. We have not seen anything that comes even close to that artistic expression. It's original in a way that you can compare to how Picasso and Van Gogh created completely new art styles while being inspired by the past. What's more, the training data that the artwork relies upon responds to its environment.
    
8.  00:03:30 --> 00:04:03
    
    The AI uses real-time data about the weather and live feed of the visitors in the museum to adapt the visualization, giving the illusion of a living being. And it does feel as if it's alive. If you ever see one of Refik Anadol's art pieces, you will have a hard time leaving. 'Unsupervised' is a captivating and aesthetic  installation that has a profound emotional effect on you when you see and hear it. However, we must remember that this installation is unsupervised only from an \*output\* point of view.
    
9.  00:04:03 --> 00:04:31
    
    To attain this level of autonomy, it has undergone extensive training by a whole team of data scientists and artists for several years. AI still relies on human processing to sort data for it to learn from. Oscar Wilde, the 19th-century playwright, famously said: "Life imitates art far more than art  imitates life." Humans have the incredible capacity to imagine and to come up with visions that do not exist anywhere on the planet.
    
10.  00:04:31 --> 00:05:01
    
    Popular culture has often inspired people to bring fictional concepts to life. Imagination is our superpower. What if we could train technology to imagine the world? Most experts think it's unlikely, at least for now. No one lives alone in a dark room all  their lives and creates something out of the blue. We are all inspired by things we see hear, smell, touch and taste. We add our spin to it to produce art. The same will be true for AI.
    
11.  00:05:01 --> 00:05:07
    
    It will always need inspiration, which will likely always come from humans.
    

## The Take Away

In design, building trust with users is paramount—especially with AI, transparency plays a pivotal role. As designers, it's essential to clearly communicate various aspects, such as the data sources, how the system learns from user data, and the probability of errors. 

A trustworthy AI system possesses several vital attributes. Firstly, it must be valid and reliable, meeting user needs and performing consistently. Safety is non-negotiable; rigorous testing is crucial to detect potential harm, and collaboration between designers, data scientists, and developers is vital to ensure user safety. 

Accountability and transparency are achieved through regular status updates and clear insights into the system's processes. Explainability and interpretability make the system understandable, aiding in debugging and monitoring. 

Privacy-enhanced AI respects users' privacy while managing biases, acknowledging the delicate balance between privacy and data inclusivity. Lastly, fairness, a nuanced concept varying across cultures, should be strived for, with careful management of biases to eliminate discrimination. As designers, understanding and implementing these principles are fundamental to crafting ethical and trustworthy AI systems.

## References and Where to Learn More

The Verge documents how **[Twitter taught Microsoft’s AI chatbot to be a racist asshole in less than a day](https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist)**. 

Vox explains how image-generating algorithms work in simple language in the video, **[AI art, explained](https://www.youtube.com/watch?v=SVcsDDABEkM)**. 

See **[Stable Diffusion Frivolous](http://www.stablediffusionfrivolous.com/)** for a teardown of the class action lawsuit against Stable Diffusion, highlighting both the distrust in AI and the complex tech that fuels the distrust.

Ars Technica’s analysis of **[Stable Diffusion copyright lawsuits](https://arstechnica.com/tech-policy/2023/04/stable-diffusion-copyright-lawsuits-could-be-a-legal-earthquake-for-ai/)**. 

Read the self-stated goals set by OpenAI, Google, Microsoft and Anthropic for the AI industry body **[Frontier Model Forum](https://techcrunch.com/2023/07/26/openai-google-microsoft-and-anthropic-form-body-to-oversee-safe-frontier-ai-development/)**.

The Scientific American’s analysis of why **[People Don't Trust AI and How We Can Change That](https://www.scientificamerican.com/article/people-dont-trust-ai-heres-how-we-can-change-that/)**.

Here’s more on the thought experiment, **[Squiggle Maximizer](https://www.lesswrong.com/tag/squiggle-maximizer-formerly-paperclip-maximizer)**.

Watch this interesting conversation on **[the opportunities of Generative AI](https://www.youtube.com/watch?v=Ht5NgznREAg)** between Mark Rolston, Ina Fried and Tom Mason at the DLD Conference.

Learn more about ***[Unsupervised,](https://refikanadol.com/works/unsupervised/)*** **[Machine Hallucinations](https://refikanadol.com/works/unsupervised/)**.